{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780d1bff-8831-4f19-8fa6-d05da8128e9a",
   "metadata": {},
   "source": [
    "#Module - 3\n",
    "# Text Wrangling\n",
    "pre-processing work that's done to prepare raw text data ready for training a machine learning model\n",
    "process of cleaning data\n",
    "It involves \n",
    "sentence splitting\n",
    "tokenization\n",
    "stemmimg and lemmatization\n",
    "stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "79334e7e-16cb-4ec1-84bc-8709d26a4133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc8eba-2782-450d-874b-ea81b097cea0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6a9034fc-e9a5-4a5f-8e5b-31cf7659f673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lets make this sample paragraph.', 'It even knows that the period in Mr. Jones is not the end.', 'Try it out!']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "samplestring = 'Lets make this sample paragraph. It even knows that the period in Mr. Jones is not the end. Try it out!'\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_sent = sent_tokenize(samplestring) #sent_tokenize recognizes thet \".\" in Mr. is not a period ending a sentence\n",
    "print(tokenized_sent)\n",
    "print(type(tokenized_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "076c5804-4363-4483-9cda-a8a885314e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lets make this sample paragraph', ' It even knows that the period in Mr', ' Jones is not the end', ' Try it out!']\n"
     ]
    }
   ],
   "source": [
    "#try another mothod to split sentences\n",
    "samplestring = 'Lets make this sample paragraph. It even knows that the period in Mr. Jones is not the end. Try it out!'\n",
    "sentences = samplestring.split(\".\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e5c72946-8b96-44f5-9e6c-95e85eb1054e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lets make this sample paragraph',\n",
       " 'It even knows that the period in Mr.',\n",
       " 'Jones is not the end',\n",
       " 'Try it out!',\n",
       " '12345678.....']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To remove spaces\n",
    "text_data = ['     Lets make this sample paragraph', ' It even knows that the period in Mr.    ', ' Jones is not the end   ', '    Try it out!', '12345678.....']\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e4559c2d-f663-452c-b8c8-c8e5414701af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lets make this sample paragraph',\n",
       " 'It even knows that the period in Mr',\n",
       " 'Jones is not the end',\n",
       " 'Try it out!',\n",
       " '12345678']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning text\n",
    "\n",
    "#Removing periodds\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6b97d528-2672-4f25-9515-84724a91d233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str, str, str, str, str]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "[capitalizer(string) for string in remove_periods]\n",
    "[type(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dc876e32-8adb-437f-90a3-5ec3f1a6635d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str, str, str, str, str]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def capitalizer(string):\n",
    "    return string.upper()\n",
    "[type(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7d3df536-d988-4dbd-a5a5-9718d14337ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ' everyone! the party ', 'starts in 10 mins. ', 'Be there ASAP!']\n"
     ]
    }
   ],
   "source": [
    "#word tokenizing\n",
    "msg = 'Hey/ everyone! the party /starts in 10 mins. /Be there ASAP!'\n",
    "print(msg.split(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b4ff3820-db7a-4999-90b8-25810cc4726d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey/', 'everyone', '!', 'the', 'party', '/starts', 'in', '10', 'mins', '.', '/Be', 'there', 'ASAP', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(msg)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "52556844-aaa9-47bf-91fb-829734d448ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey/everyone!theparty/startsin10mins./BethereASAP!'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=\"\"\n",
    "for word in tokenized_word:\n",
    "    new+=word\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f8dcdce9-5684-4158-9652-a49e63043fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey/everyone!theparty/startsin10mins./BethereASAP!'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "712ca4be-6708-4518-84e2-68844984eadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7951ace9-653e-4140-a20c-0049c4cd2142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9cbe95a2-a4a7-4636-aa0d-3ff60af32a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXX XXXX XXXX XXXXXX XXXXXXXXX',\n",
       " 'XX XXXX XXXXX XXXX XXX XXXXXX XX XX',\n",
       " 'XXXXX XX XXX XXX XXX',\n",
       " 'XXX XX XXX!',\n",
       " '12345678']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d253dcc4-7feb-499b-84d1-f15811bfd57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization\n",
    "#. converting the word to root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9549f7a7-711a-4387-8afe-47285cf72152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "faaa8ead-2766-4372-ae00-aa59e27e00bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7e17557f-7f30-4a63-85ae-d1fdbc9e33b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ate'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('ate') #expected to see eat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "df36c2a2-4d5d-45c0-8fea-329c108afdf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sat'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('sat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7d09c2f2-972f-4d39-856a-841268a35393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('sitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d1926b54-5237-438c-a006-0016598742a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0873bbf9-c984-460c-9e45-446032085700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'majest'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('majestic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2d32023b-4d6d-47d7-80a9-0ee26cfc88c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'energ'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('energized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "939d9450-a31f-41fe-9fc2-f0d100406617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'program'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('Programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bfeb592b-4076-4650-a0e8-eadcf65a2a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('liking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "890ff996-cfbb-4c3b-a142-56b9338e562e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('Stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "10d6ad3f-1fd8-4811-9d36-db3b3ae82af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('Universe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "769a4344-ff46-4ad7-8cbd-ecdeacbd125b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('University')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "52b67405-64cb-418c-97c8-5a6dcc1f6d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('Universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ed4a4e8d-4ef0-4248-a18a-977aaa727860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here these are differnt words with completely different meaning. Stemming returned a same root for both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae8e1e-ffb5-4790-88a0-18f6a553de3c",
   "metadata": {},
   "source": [
    "# Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "73096d14-9dd5-455d-9182-6faf20de92b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Python NLTK provides WordNet Lemmatizer that uses the WordNet Db to get lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "68a9b93f-f618-4913-b743-5b5dfa363e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c6421144-f072-4acb-9bbf-8d168e119991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize(\"running\", pos = \"n\")) # parts of speech(pos) #n = noun\n",
    "print(lem.lemmatize(\"running\", pos = \"v\")) # v- verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "96a06a54-d30f-4e4d-932a-f2e02de196bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize(\"better\", pos = \"a\"))\n",
    "print(lem.lemmatize(\"ate\", pos = \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6088a000-0217-4d32-8252-003c5abb88aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful\n",
      "universal\n",
      "sit\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize(\"beautiful\", pos = \"a\"))\n",
    "print(lem.lemmatize(\"universal\", pos = \"n\"))\n",
    "print(lem.lemmatize(\"sat\", pos = \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3116383a-be6a-4023-beca-9ff3d56fc593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard\n",
      "harder\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize(\"harder\", pos = \"a\"))\n",
    "print(lem.lemmatize(\"harder\", pos = \"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d9fcf-1386-46dc-ba10-edc0d8f05603",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "999056bf-e610-4465-9f5d-6202d3050157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a an the in is otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9c419469-3d45-41f1-b71c-c44577a22f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5704adfd-3932-47ce-8ce5-8a6224f55940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9ce800e1-6886-4841-b5de-f7bb313c44e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"A sentence is a textual unit consisting of words that are grammatically linked.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "434477d4-9ef3-40bf-810d-d712c009730d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'textual',\n",
       " 'unit',\n",
       " 'consisting',\n",
       " 'of',\n",
       " 'words',\n",
       " 'that',\n",
       " 'are',\n",
       " 'grammatically',\n",
       " 'linked',\n",
       " '.']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(text.lower())\n",
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f16cf364-c0d0-408e-bfd6-e3e07ca9a90c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence',\n",
       " 'textual',\n",
       " 'unit',\n",
       " 'consisting',\n",
       " 'words',\n",
       " 'grammatically',\n",
       " 'linked',\n",
       " '.']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop word are all lower case so convert text to lower before using sto[p words otherwise stopwords won't be removed\n",
    "\n",
    "filtered_words = []\n",
    "for token in tokenized_word:\n",
    "    if(token not in stopwords):\n",
    "        filtered_words.append(token)\n",
    "filtered_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ed8ad439-7b00-4c1c-ac59-412afe86ddc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Part of Speech\n",
    "import nltk\n",
    "text = \"can you please but me an Arizona Ice Tea? It is $0.99.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1ccbab76-7622-469e-b4c4-aca92cded4be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can',\n",
       " 'you',\n",
       " 'please',\n",
       " 'but',\n",
       " 'me',\n",
       " 'an',\n",
       " 'arizona',\n",
       " 'ice',\n",
       " 'tea',\n",
       " '?',\n",
       " 'it',\n",
       " 'is',\n",
       " '$',\n",
       " '0.99',\n",
       " '.']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c11ff31c-c2b3-4d16-bf4b-d028031efee4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of speech: [('can', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('but', 'CC'), ('me', 'PRP'), ('an', 'DT'), ('arizona', 'JJ'), ('ice', 'NN'), ('tea', 'NN'), ('?', '.'), ('it', 'PRP'), ('is', 'VBZ'), ('$', '$'), ('0.99', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"part of speech:\", nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a731de67-064e-443a-964e-012dd4111cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regular Expression\n",
    "import re\n",
    "tweet = 'Hello guys, this is my first #tweet. Check this @joyi #nlp #machine #learning'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0baa543d-1c06-4918-873d-bed0d9fab757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', '#', '#', '#']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('#', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b8c8d8f8-d080-47bb-a42d-967360d47197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(29, 30), match='#'>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"#\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b23cf199-c682-4a1f-9123-24d475407ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello guys, this is my first _tweet. Check this @joyi _nlp _machine _learning'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('#', \"_\",tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a1336e14-5b6f-4981-a5df-3fa243d0971a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(48, 49), match='@'>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"@\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ef68f53a-8ffe-47e2-b95d-ce0778dc8f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(12, 16), match='this'>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"this\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "81ff46a6-07b5-4384-bf86-2698ce8918ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(77, 77), match=''>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"$\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c7b5cbba-797c-4ad1-8848-5aa4b07fa506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "re.match(\"#\",tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "21940192-915d-4dd2-8f98-d665eb7d8b14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello guys, this is my first #tweet. Check this $joyi #nlp #machine #learning'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"@\", \"$\",tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927539c-785b-4ad1-80c9-55f893d289f3",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cdfc5e29-d1ce-4433-91f9-16dba58dea44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature engineering - Recurrsive Feature Elimination (RFE)  Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cb92fe5b-a991-4c5c-955a-c06fe8e10ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Vocabulary from all the unique word\n",
    "#Drawbacks of BoW\n",
    "# -> repetitive words with no variability\n",
    "# -> new words\n",
    "# -> Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1c8cbc2e-c146-441e-b0ea-53383dadd4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'germany beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e9dddabd-fc2d-4128-908b-c06d67746203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "bag_of_words #the output is a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "83bfa751-e227-4007-9f47-abc0b86a02ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cf = count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f7452a44-8052-4885-ab98-92070cdc6edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOG = bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b84940f7-53c3-4ac2-8849-aadcfa790682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(BOG, columns = cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e525b06d-9ee5-46f7-b69d-d3846d57233f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beats</th>\n",
       "      <th>best</th>\n",
       "      <th>both</th>\n",
       "      <th>brazil</th>\n",
       "      <th>germany</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sweden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beats  best  both  brazil  germany  is  love  sweden\n",
       "0      0     0     0       2        0   0     1       0\n",
       "1      0     1     0       0        0   1     0       1\n",
       "2      1     0     1       0        1   0     0       0"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd2eb9-fdf2-42b1-b2d8-63613dbda609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
